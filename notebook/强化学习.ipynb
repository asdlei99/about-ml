{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、强化学习\n",
    "### 术语\n",
    "\n",
    "- 智体(agent)\n",
    "- 环境(Environment)\n",
    "- 状态(State) s \n",
    "- 动作(Action) a \n",
    "- 策略(Policy) π \n",
    "- 奖励(Reward) r \n",
    "- 状态转移概率 T\n",
    "\n",
    "    <img align=\"left\" src=\"image\\模型.png\" width=400>\n",
    "\n",
    "### 基本要素\n",
    "- 环境\n",
    "- 回报函数\n",
    "- 值函数\n",
    "- 策略\n",
    "    \n",
    "    <img align=\"left\" src=\"image\\基本要素.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、强化学习理论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔科夫过程\n",
    "- 马尔可夫过程（Markov process）是一类**随机过程**。它的原始模型**马尔可夫链**，由俄国数学家A.A.马尔可夫于1907年提出。\n",
    "  \n",
    "  *⑴ 随机过程*:\n",
    "  \n",
    "  *⑵ 马尔科夫链*:\n",
    "  \n",
    "- 马尔可夫过程是研究离散事件动态系统状态空间的重要方法，它的数学基础是随机过程理论。\n",
    "- 若随机过程满足**马尔可夫性**，则称为马尔可夫过程。\n",
    "\n",
    "  *⑴ 马尔可夫性*：设  为一随机过程，E为其状态空间，若对任意的  ，任意的  ，随机变量X(t)在已知变量  之下的条件分布函数只与  有关，而与  无关，即条件分布函数满足等式\n",
    "\n",
    "<img src=\"https://gss0.bdstatic.com/-4o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D392/sign=c63760d068d0f703e2b293d53afb5148/5ab5c9ea15ce36d3991db5f730f33a87e850b1df.jpg\">\n",
    "\n",
    "        即\n",
    "\n",
    "<img src=\"https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D440/sign=d4b9dad08e35e5dd942ca4db46c7a7f5/a9d3fd1f4134970aab352aad9ecad1c8a6865de9.jpg\">\n",
    "\n",
    "        此性质称为马尔可夫性，亦称**无后效性或无记忆性**。\n",
    "\n",
    "### 马尔科夫决策过程(MDP)\n",
    "- **马尔可夫决策过程**是基于马尔可夫过程理论的随机动态系统的最优决策过程。马尔可夫决策过程是序贯决策的主要研究领域。它是马尔可夫过程与确定性的动态规划相结合的产物，故又称马尔可夫型随机动态规划，属于运筹学中数学规划的一个分支。\n",
    "\n",
    "    - *马尔可夫决策过程*是一个五元组  ，其中\n",
    "        1. S是一组有限的状态;\n",
    "        2. A是一组有限的行为（或者，Аs是从状态可用的有限的一组行动  ）;\n",
    "        3. <img src=\"https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D274/sign=776f195489025aafd73279cccfecab8d/060828381f30e9241ee5c3a547086e061c95f7e5.jpg\">是行动的概率a在状态s在时间t会导致状态s'在时间  ;\n",
    "        4. <img src=\"https://gss1.bdstatic.com/-vo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D53/sign=1a068380b399a9013f355b351c958d88/2fdda3cc7cd98d1004df53a52a3fb80e7bec901e.jpg\">是从国家转型后得到的直接奖励（或期望的直接奖励）；\n",
    "        5. <img src=\"https://gss2.bdstatic.com/9fo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D61/sign=8a5c66503aadcbef05347d07aeafd7d2/fd039245d688d43fdd6ace66761ed21b0ff43b66.jpg\">是折现系数，代表未来奖励与现在奖励之间的重要差异；\n",
    "    \n",
    "<img src=\"https://gss2.bdstatic.com/-fo3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike116%2C5%2C5%2C116%2C38/sign=a563bb59f7039245b5b8e95de6fdcfa7/0eb30f2442a7d933ae38a9e7a64bd11372f00195.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于模型的动态规划(DP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型未知的强化学习方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 资料\n",
    "1. 《Reinforcement Learning: An Introduction》，http://incompleteideas.net/book/the-book-2nd.html\n",
    "2. 《Reinforcement Learning》，https://cn.udacity.com/course/reinforcement-learning--ud600"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
